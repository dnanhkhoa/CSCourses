{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:24:07.534018Z",
     "start_time": "2017-04-15T23:24:07.460688+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# BÁO CÁO BÀI TẬP 1:\n",
    "\n",
    "** CÀI ĐẶT MẠNG FEED FORWARD ĐỂ DỰ ĐOÁN KÍ SỐ VIẾT TAY (MNIST) **\n",
    "\n",
    "\n",
    "** THÔNG TIN NHÓM **\n",
    "\n",
    "|Họ tên|Mã số sinh viên|\n",
    "|---|--:|\n",
    "|Bùi Duy Đăng|1312127|\n",
    "|Mai Hoàng Hưng|1312268|\n",
    "|Dương Nguyễn Anh Khoa|1312288|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Kế hoạch\n",
    "\n",
    "** Theo thứ tự ưu tiên: **\n",
    "\n",
    "|Thứ tự|Tên|Công việc phân công|\n",
    "|---|---|---|\n",
    "|1|Khoa|Trình bày mã giả của mô hình Feed forward trong báo cáo.|\n",
    "|2|Khoa|Visualize dữ liệu.|\n",
    "|3|Hưng|Cài đặt mô hình Feed forward không sử dụng thư viện.|\n",
    "|4|Đăng|Cài đặt mô hình Feed forward sử dụng thư viện TensorFlow để đối chiếu.|\n",
    "|5|Khoa|Tham khảo cách chứng minh công thức, trình bày lại báo cáo.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dữ liệu huấn luyện\n",
    "\n",
    "Nhóm sử dụng tập dữ liệu kí số viết tay MNIST của nhóm tác giả: Yann LeCun, Corinna Cortes, Christopher J.C. Burges.\n",
    "\n",
    "Tập dữ liệu là các ảnh và nhãn của 10 nhóm kí số khác nhau bao gồm: 60,000 mẫu huấn luyện (training set) và 10,000 mẫu kiểm thử (test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:23:48.593907Z",
     "start_time": "2017-04-15T23:23:48.547290+07:00"
    },
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sơ lược về mô hình Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Cấu trúc mô hình mạng Feed Forward **\n",
    "![Feed Forward](http://machinelearningcoban.com/assets/14_mlp/ex_nn.png)\n",
    "\n",
    "Nguồn ảnh: http://machinelearningcoban.com\n",
    "\n",
    "Trong đó, có 4 thành phần có khả năng học là: $\\mathbf{W}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{B}^{(1)}, \\mathbf{B}^{(2)}$ với $\\mathbf{W}^{(l)}$ là bộ trọng số thể hiện các kết nối từ layer thứ $l - 1$ đến layer thứ $l$ và $\\mathbf{B}^{(l)}$ là bias của layer thứ $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Các kí hiệu **\n",
    "- N: số mẫu dữ liệu\n",
    "- $\\mathbf{X}$: tập đặc trưng đầu vào\n",
    "- $\\mathbf{Y}$: tập nhãn đầu vào\n",
    "- C: Số lớp (trong MNIST thì C = 10)\n",
    "- $\\lambda$: hệ số cân bằng dùng cho regularization (weight decay)\n",
    "- $\\eta$: tỉ lệ học (learning rate)\n",
    "- $d^{(l)}$: số neurons của layer $l$\n",
    "- $\\mathbf{W}^{(l)}$: bộ trọng số thể hiện các kết nối từ layer thứ $l - 1$ đến layer thứ $l$\n",
    "- $\\mathbf{W}_{ij}^{(l)}$: trọng số thể hiện kết nối từ neuron j của layer thứ $l - 1$ đến neuron i của layer thứ $l$\n",
    "- $\\mathbf{B}^{(l)}$: bias của layer thứ $l$\n",
    "- $\\mathbf{Z}^{(l)}$: giá trị đầu vào của các neurons thuộc layer thứ $l$\n",
    "- $\\mathbf{A}^{(l)}$: giá trị đầu ra sau khi qua hàm kích hoạt của các neurons thuộc layer thứ $l$\n",
    "- $\\mathbf{E}^{(l)}$: độ lỗi của layer thứ $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Hàm kích hoạt **\n",
    "\n",
    "Nhóm sử dụng hàm **sigmoid** làm hàm kích hoạt vì nó khá thông dụng trước đây và dễ cài đặt cho các bài toán nhỏ, hơn nữa nó rất dễ tính đạo hàm (giả sử không quan tâm đến nhược điểm gradient của hàm xấp xỉ 0 khi giá trị đầu vào có trị tuyệt đối rất lớn).\n",
    "\n",
    "Công thức: $\\sigma(z) = \\frac{1}{(1 + e^{(-z)})}$\n",
    "\n",
    "Và đạo hàm của nó: $\\frac{\\partial \\sigma (z)}{\\partial (z)} = \\sigma (z)\\cdot (1-\\sigma(z)) ~~~ (*)$\n",
    "\n",
    "*Để có thể rút gọn khai triển của đạo hàm, cộng thêm 1 để rút gọn.*\n",
    "\n",
    "Bên cạnh đó, do đây là bài toán classification nên nhóm sử dụng hàm kích hoạt Softmax cho output layer để tính xác suất một điểm dữ liệu rơi vào lớp nào (nhóm sử dụng phiên bản cải tiến để tránh trường hợp tràn số khi $z_i$ quá lớn bằng cách trừ $z_i$ cho 1 hằng số c nào đó).\n",
    "\n",
    "Công thức gốc: $a_i = \\text{softmax}(z_i) = \\frac{e^{(z_i)}}{\\sum_{j=1}^C e^{(z_j)}}, ~~ \\forall i = 1, 2, \\dots, C$\n",
    "\n",
    "Trong đó, $\\sum_{i=1}^C a_i = 1$ và không tồn tại $a_i$ nào tuyệt đối bằng 0 hoặc bằng 1 (chỉ có xấp xỉ) vì tử số là $e^{z_i}$ nên đảm bảo khác 0. Nhưng tử số lại có thể là 1 nếu $z_i=0$, điều này không đáng lo bởi vì tồn tại $z_j > 0, j \\neq i$ để đảm bảo khi chia cho mẫu sẽ cho ra $a_i < 1, ~~ \\forall i=1, 2, \\dots, C.$\n",
    "\n",
    "Công thức cải tiến: $a_i = \\text{softmax}(z_i) = \\frac{e^{(z_i-c)}}{\\sum_{j=1}^C e^{(z_j-c)}}, ~~ \\forall i = 1, 2, \\dots, C$\n",
    "\n",
    "Thực nghiệm cho thấy, giá trị c đủ lớn được xác định bằng $c = max(z_i)$.\n",
    "\n",
    "**Đạo hàm Softmax**\n",
    "\n",
    "Để có thể sử dụng hàm softmax làm hàm kích hoạt cho output layer, chúng ta cần tính đạo hàm của nó giống như hàm sigmoid.\n",
    "\n",
    "*Qui ước: $\\Sigma_C = \\sum_{i=1}^C \\exp(z_i), ~~ \\forall i = 1, 2, \\dots, C$*\n",
    "\n",
    "Ta tính đạo hàm như sau:\n",
    "\n",
    "*Do tính đạo hàm theo $z$ nên sẽ có 2 trường hợp $i = j$ và $i \\neq j$*\n",
    "\n",
    "\\begin{split}\n",
    "\\text{Nếu} \\; i = j :& \\frac{\\partial a_i}{\\partial z_i} = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma_C}}{\\partial z_i} = \\frac{e^{z_i}\\Sigma_C - e^{z_i}e^{z_i}}{\\Sigma_C^2} = \\frac{e^{z_i}}{\\Sigma_C}\\frac{\\Sigma_C - e^{z_i}}{\\Sigma_C} = \\frac{e^{z_i}}{\\Sigma_C}(1-\\frac{e^{z_i}}{\\Sigma_C}) =  a_i (1 - a_i) ~~~ (**)\\\\\n",
    "\\text{Nếu} \\; i \\neq j :& \\frac{\\partial a_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma_C}}{\\partial z_j} = \\frac{0 - e^{z_i}e^{z_j}}{\\Sigma_C^2} = -\\frac{e^{z_i}}{\\Sigma_C} \\frac{e^{z_j}}{\\Sigma_C} = -a_i a_j ~~~ (***)\n",
    "\\end{split}\n",
    "\n",
    "*Khai triển trên được tham khảo từ: http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Hàm chi phí Cross Entropy và tính Gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Có 2 phân bố xác suất gồm $\\mathbf{y}$ và $\\mathbf{a}$. Chúng ta cần tìm 1 hàm sao cho đạt cực tiểu khi $\\mathbf{y} = \\mathbf{a} $ và khi $\\mathbf{a}$ càng xa $\\mathbf{y}$ thì hàm phải cho giá trị càng lớn (rất lớn).\n",
    "\n",
    "Do đó, hàm cross entropy $H(\\mathbf{y}, \\mathbf{a}) =-\\sum_{i=1}^C y_i \\log a_i$ được sử dụng vì thỏa mãn những yêu cầu trên.\n",
    "\n",
    "Có một điểm quan trọng là hàm cross entropy không có tính đối xứng. Có thể nhận thấy rằng giá trị $\\mathbf{a_i}$ trong $\\log a_i$ bắt buộc phải lớn hơn 0 bởi vì $\\log(0)$ không xác định. Ngược lại, $\\mathbf{y}$ không bị ràng buộc như thế. Do đó, khi cài đặt hàm cross entropy cần quan tâm đến thứ tự của chúng.\n",
    "\n",
    "Trong bài toán này, sinh viên sẽ sử dụng hàm chi phí cross entropy sau cho đầu ra của lớp softmax (có thể hiểu là một phân bố xác suất).\n",
    "\n",
    "$J(\\mathbf{W}; \\mathbf{X}, \\mathbf{Y}) = -\\frac{1}{N}\\sum_{i = 1}^N \\sum_{j = 1}^C y_{ji}\\log(\\hat{y}_{ji}) + \\frac{\\lambda}{2N}\\sum_{i = 1}^{d^{(l)}}\\sum_{j = 1}^{d^{(l + 1)}}(\\mathbf{W}_{ji}^{(l)})^2$\n",
    "\n",
    "*Trong hàm chi phí trên có sử dụng thêm một đại lượng regularization dùng để \"phạt\" mô hình nhằm hạn chế overfitting.*\n",
    "\n",
    "Kế đến, cần khai triển đạo hàm hàm chi phí theo từng $\\mathbf{W}$ và $\\mathbf{B}$ của mỗi layer để có thể sử dụng Gradient Descent:\n",
    "$\\frac{\\partial J}{\\partial \\mathbf{W}^{(l)}} ; \\frac{\\partial J}{\\partial \\mathbf{b}^{(l)}},~~ l = 1, 2, \\dots, L$\n",
    "\n",
    "*Để đơn giản, các khai triển dưới đây thực hiện trên chiến lược Stochastic Gradient Descent.*\n",
    "\n",
    "Đầu tiên, đạo hàm hàm chi phí theo từng thành phần của ma trận trọng số output layer:\n",
    "\n",
    "\\begin{split}\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(L)}} &=& \\frac{\\partial J}{\\partial z_i^{(L)}}. \\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}} ~~~ (1)\n",
    "\\end{split}\n",
    "\n",
    "Sử dụng quy tắc chain rule để khai triển đạo hàm. Trong đó gồm có 2 thành phần. Sinh viên sẽ lần lượt khai triển tiếp theo từng thành phần.\n",
    "\n",
    "*Để đơn giản khi gõ công thức, phần khai triển bên dưới sẽ được hiểu ngầm là đạo hàm theo các thành phần của output layer.*\n",
    "\n",
    "\\begin{split}\n",
    "\\frac{\\partial J}{\\partial z_i} & = - \\sum_{j=1}^C \\frac{\\partial y_j log(\\hat{y}_j)}{\\partial z_i}{} =\n",
    "- \\sum_{j=1}^C y_j \\frac{\\partial log(\\hat{y}_j)}{\\partial z_i} = - \\sum_{j=1}^C y_j \\frac{1}{\\hat{y}_j} \\frac{\\partial \\hat{y}_j}{\\partial z_i} \\\\\n",
    "& = - \\frac{y_i}{\\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_i} - \\sum_{j \\neq i}^C \\frac{y_j}{\\hat{y}_j} \\frac{\\partial \\hat{y}_j}{\\partial z_i}\n",
    "= - \\frac{y_i}{\\hat{y}_i} \\hat{y}_i (1-\\hat{y}_i) - \\sum_{j \\neq i}^C \\frac{y_j}{\\hat{y}_j} (-\\hat{y}_j \\hat{y}_i) \\\\\n",
    "& = - y_i + y_i \\hat{y}_i + \\sum_{j \\neq i}^C y_j \\hat{y}_i = - y_i + \\sum_{j = 1}^C y_j \\hat{y}_i\n",
    "= -y_i + \\hat{y}_i \\sum_{j = 1}^C y_j \\\\\n",
    "& = \\hat{y}_i - y_i, ~~~ \\forall i \\in C ~~~ (2)\n",
    "\\end{split}\n",
    "\n",
    "*Chúng ta dùng lại khai triển $\\text{(**)}$  và $\\text{(***)}$ vào dòng 2. Bên cạnh đó, lượng regularization sẽ không còn khi đạo hàm theo $z_i$.*\n",
    "\n",
    "*Khai triển trên được tham khảo từ: http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/*\n",
    "\n",
    "Kế đến là phần còn lại: $\\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}}$\n",
    "\n",
    "\\begin{split}\n",
    "\\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}} & = \\frac{\\partial \\mathbf{w}_i^{(L)T}\\mathbf{a}^{(L-1)} + b_i^{(L)}}{\\partial w_{ij}^{(L)}} \\\\\n",
    "& = a_j^{(L-1)} ~~~ (3)\n",
    "\\end{split}\n",
    "\n",
    "Thay (2) (3) vào (1), ta được:\n",
    "\n",
    "\\begin{split}\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(L)}} & = \\frac{\\partial J}{\\partial z_i^{(L)}}. \\frac{\\partial z_i^{(L)}}{\\partial w_{ij}^{(L)}} \\\\\n",
    "& = (\\hat{y}_i - y_i)a_j^{(L-1)} \\\\\n",
    "& = e_i^{(L)} a_j^{(L-1)} ~~~ (4)\n",
    "\\end{split}\n",
    "\n",
    "*Đặt $e_i^{(L)}=(\\hat{y}_i - y_i)$ với $L$ là output layer để rút gọn.*\n",
    "\n",
    "Tương tự, đạo hàm của hàm chi phí theo bias của output layer là:\n",
    "\n",
    "\\begin{split}\n",
    "\\frac{\\partial J}{\\partial b_{i}^{(L)}} = \\frac{\\partial J}{\\partial z_i^{(L)}}. \\frac{\\partial z_i^{(L)}}{\\partial b_{i}^{(L)}} = e_i^{(L)} ~~~ (5)\n",
    "\\end{split}\n",
    "\n",
    "Các khai triển bên trên chỉ mới là tính cho output layer, các layer trước đó sẽ được tính dựa vào layer sau nó. Do đó mới được gọi là Back propagation.\n",
    "\n",
    "Ở các layer $l$ trước đó, độ lỗi $e$ được tính như sau:\n",
    "\n",
    "\\begin{split}\n",
    "e_i^{(l)} &=& \\frac{\\partial J}{\\partial z_i^{(l)}} = \\frac{\\partial J}{\\partial a_i^{(l)}} . \\frac{\\partial a_i^{(l)}}{\\partial z_i^{(l)}} \\\\\n",
    "&=& \\left( \\sum_{k = 1}^{d^{(l+1)}} \\frac{\\partial J}{\\partial z_k^{(l+1)}} .\\frac{\\partial z_k^{(l+1)}}{\\partial a_i^{(l)}} \\right) f’(z_i^{(l)}) \\\\\n",
    " &=&\\left( \\sum_{k = 1}^{d^{(l+1)}} e_k^{(l+1)} w_{ki}^{(l+1)} \\right) f’(z_i^{(l)}) \\\\\n",
    " &=&\\left( \\mathbf{w}_{i:}^{(l+1)} \\mathbf{e}^{(l+1)} \\right) f’(z_i^{(l)}) ~~~ (6)\n",
    "\\end{split}\n",
    "\n",
    "Trong đó:\n",
    "- $f’$ là đạo hàm của sigmoid\n",
    "- $\\mathbf{w}_{i:}^{(l+1)}$ là hàng thứ i của ma trận trọng số $\\mathbf{W}^{(l+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Bước Feed Forward **\n",
    "\n",
    "Đầu tiên, khởi tạo ngẫu nhiên cho các ma trận trọng số và bias.\n",
    "\n",
    "Kế đến, tính giá trị cho các neurons ở hidden layer:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{Z}^{(1)} &=& \\mathbf{W}^{(1)T}\\mathbf{X} + \\mathbf{B}^{(1)} \\\\\n",
    "\\mathbf{A}^{(1)} &=& \\sigma (\\mathbf{Z}^{(1)})\n",
    "\\end{split}\n",
    "\n",
    "Sau đó, tính giá trị cho các neurons ở output layer:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{Z}^{(2)} &=& \\mathbf{W}^{(2)T}\\mathbf{A}^{(1)} + \\mathbf{B}^{(2)} \\\\\n",
    "\\mathbf{\\hat{Y}} = \\mathbf{A}^{(2)} &=& \\text{softmax}(\\mathbf{Z}^{(2)})\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bước Back Backpropagation **\n",
    "\n",
    "Đầu tiên, tính độ lỗi ở output layer theo như cách đã khai triển ở (2) và sau đó tính gradient theo từng thành phần của bộ trọng số thuộc output layer gồm $\\mathbf{W}^{(2)}$ và $\\mathbf{B}^{(2)}$:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{E}^{(2)} &=& \\frac{\\partial J}{\\partial \\mathbf{Z}^{(2)}} =\\frac{1}{N}(\\mathbf{\\hat{Y}} - \\mathbf{Y}) \\\\\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} &=& \\mathbf{A}^{(1)}  \\mathbf{E}^{(2)T} \\\\\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}} &=& \\sum_{n=1}^N\\mathbf{e}_n^{(2)}\n",
    "\\end{split}\n",
    "\n",
    "Khi đã tính xong gradient thì cập nhật vào bộ trọng số:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{W}^{(2)} = \\mathbf{W}^{(2)} - \\eta\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} \\\\\n",
    "\\mathbf{B}^{(2)} = \\mathbf{B}^{(2)} - \\eta\\frac{\\partial J}{\\partial \\mathbf{B}^{(2)}}\n",
    "\\end{split}\n",
    "\n",
    "Kế đến, tính độ lỗi ở hidden layer theo như cách đã khai triển ở (6) và sau đó tính gradient theo từng thành phần của bộ trọng số thuộc hidden layer gồm $\\mathbf{W}^{(1)}$ và $\\mathbf{B}^{(1)}$:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{E}^{(1)} &=& \\left(\\mathbf{W}^{(2)}\\mathbf{E}^{(2)}\\right) \\odot f’(\\mathbf{Z}^{(1)}) \\\\\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} &=& \\mathbf{A}^{(0)}  \\mathbf{E}^{(1)T} = \\mathbf{X}\\mathbf{E}^{(1)T}\\\\\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}} &=& \\sum_{n=1}^N\\mathbf{e}_n^{(1)} \\\\\n",
    "\\end{split}\n",
    "\n",
    "Khi đã tính xong gradient thì cập nhật vào bộ trọng số:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{W}^{(1)} = \\mathbf{W}^{(1)} - \\eta\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} \\\\\n",
    "\\mathbf{B}^{(1)} = \\mathbf{B}^{(1)} - \\eta\\frac{\\partial J}{\\partial \\mathbf{B}^{(1)}}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Chiến lược học **\n",
    "\n",
    "Sử dụng chiến lược Mini-batch Gradient Descent để tăng tốc độ học và hạn chế bị thiếu bộ nhớ khi huấn luyện.\n",
    "\n",
    "Chiến lược này chọn ra tập con của dữ liệu (n mẫu, n > 1) để tính xấp xỉ Gradient rồi mới cập nhật bộ trọng số $\\mathbf{W}$.\n",
    "\n",
    "Công thức cập nhật: $\\mathbf{W} = \\mathbf{W} - \\eta\\nabla_{\\mathbf{W}} J(\\mathbf{W}; \\mathbf{x}_{i:i+n}; \\mathbf{y}_{i:i+n})$, với $i$ là mẫu thứ $i$.\n",
    "\n",
    "Bên cạnh đó, nhóm cũng sử dụng giá trị \\textbf{learning rate} động để tránh trường hợp mô hình khó hoặc không thể hội tụ.\n",
    "\n",
    "Giá trị learning rate được tính bằng công thức sau: Tỉ lệ học $ = \\frac{LEARNING\\_RATE}{t + 1}$ với $t$ là giá trị epoch hiện tại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Mã nguồn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Khai báo các thư viện cần thiết cho các đoạn code bên dưới:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T15:59:03.523288Z",
     "start_time": "2017-04-15T22:58:53.575363+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "# Số lớp trong dataset, gồm các nhãn được đánh số từ 0 đến 9 ứng với 10 chữ số\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Mỗi ảnh trong dataset có kích thước 28x28 -> vector của ảnh có kích thước 784x1\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Đọc dữ liệu huấn luyện lên bộ nhớ và chuyển nhãn của ảnh thành one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:01:01.917045Z",
     "start_time": "2017-04-15T23:01:00.693542+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/train-images-idx3-ubyte.gz\n",
      "Extracting dataset/train-labels-idx1-ubyte.gz\n",
      "Extracting dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting dataset/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Đọc dữ liệu huấn luyện từ thư mục, nhãn được biểu diễn dưới dạng one-hot vector\n",
    "mnist = read_data_sets(\"dataset/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Để có thể quan sát dữ liệu dễ dàng, sinh viên cho hiển thị ngẫu nhiên 10 lớp ảnh trong dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:01:31.269684Z",
     "start_time": "2017-04-15T23:01:31.248666+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_dataset(NUM_EXAMPLES=10):\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        # Tìm danh sách indices ứng với lớp cls\n",
    "        idxs = np.where(mnist.train.labels.argmax(axis=1) == cls)[0]\n",
    "        # Chọn ra k ảnh ứng với lớp cls (không hoàn lại)\n",
    "        idxs = np.random.choice(idxs, NUM_EXAMPLES, replace=False)\n",
    "        for i, idx in enumerate(idxs):\n",
    "            plt.subplot(NUM_EXAMPLES, NUM_CLASSES, i * NUM_CLASSES + cls + 1)\n",
    "            plt.imshow(\n",
    "                mnist.train.images[idx].reshape((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                cmap=matplotlib.cm.binary)\n",
    "\n",
    "            plt.axis('off')\n",
    "            if i == 0:\n",
    "                plt.title(cls + 1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:01:44.371151Z",
     "start_time": "2017-04-15T23:01:36.001522+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEICAYAAACZJtWMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0f46e3630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mô hình Feed Forward sử dụng thư viện TensorFlow\n",
    "\n",
    "** Mục đích: đối chiếu độ chính xác so với mô hình không sử dụng thư viện **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:03:48.920397Z",
     "start_time": "2017-04-15T23:03:48.807084+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feedforward_with_tensorflow(LEARNING_RATE=0.15,\n",
    "                                LAMBDA=0.001,\n",
    "                                NUM_EPOCHES=10,\n",
    "                                BATCH_SIZE=64,\n",
    "                                NUM_NEURONS=625):\n",
    "    \n",
    "    # Khai báo kích thước image vector và label vector\n",
    "    X = tf.placeholder(tf.float32, [None, IMAGE_PIXELS])\n",
    "    Y = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "    # Khai báo bộ trọng số W và bias của hidden layer và output layer\n",
    "    Wh = tf.Variable(tf.random_normal([IMAGE_PIXELS, NUM_NEURONS]))\n",
    "    Bh = tf.Variable(tf.ones([NUM_NEURONS]))\n",
    "    Wo = tf.Variable(tf.random_normal([NUM_NEURONS, NUM_CLASSES]))\n",
    "    Bo = tf.Variable(tf.ones([NUM_CLASSES]))\n",
    "\n",
    "    # Truyền giá trị của các neurons trong lớp ẩn qua hàm phi tuyến sigmoid\n",
    "    Ah = tf.nn.sigmoid(tf.add(tf.matmul(X, Wh), Bh))\n",
    "    # Do lớp output sử dụng softmax nên không truyền qua hàm sigmoid\n",
    "    Y_pred = tf.add(tf.matmul(Ah, Wo), Bo)\n",
    "\n",
    "    # Khai báo hàm loss là hàm softmax cross entropy dùng để tính khoảng cách giữa hai phân bố xác suất + softmax\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y))\n",
    "\n",
    "    # Sử dụng L2 Regularization để hạn chế overfitting\n",
    "    regularizer = (tf.nn.l2_loss(Wh) + tf.nn.l2_loss(Wo)) / BATCH_SIZE\n",
    "    loss += LAMBDA * regularizer\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "\n",
    "    # Khai báo sử dụng phương pháp Gradient Descent để cực tiểu hàm loss\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss)\n",
    "\n",
    "    # Khởi tạo Graph session trong TensorFlow\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Khởi tạo các biến trong Graph\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Lặp qua từng epoch để tính gradient\n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        avg_loss = 0.\n",
    "        num_batches = mnist.train.num_examples // BATCH_SIZE\n",
    "        for i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            _, l = sess.run(\n",
    "                [train_step, loss],\n",
    "                feed_dict={\n",
    "                    learning_rate: LEARNING_RATE / (epoch + 1),\n",
    "                    X: batch_xs,\n",
    "                    Y: batch_ys\n",
    "                })\n",
    "\n",
    "            avg_loss += l / num_batches\n",
    "        print('Epoch: %d - Average loss: %f' % (epoch + 1, avg_loss))\n",
    "\n",
    "    # Tính độ chính xác của mô hình\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_pred, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Accuracy: %f' % sess.run(\n",
    "        accuracy, feed_dict={X: mnist.test.images,\n",
    "                             Y: mnist.test.labels}))\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mô hình Feed Forward không sử dụng thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:09:18.811623Z",
     "start_time": "2017-04-15T23:09:18.628171+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "# Hàm softmax cải tiến hạn chế tràn số khi z quá lớn (trừ z cho 1 giá trị c nào đó)\n",
    "# Bài Softmax Regression của anh Vũ Hữu Tiệp (http://machinelearningcoban.com)\n",
    "def softmax(z):\n",
    "    ez = np.exp(z - np.max(z, axis = 1, keepdims = True))\n",
    "    prob = ez / ez.sum(axis = 1, keepdims=True)\n",
    "    return prob\n",
    "\n",
    "\n",
    "def feedforward(LEARNING_RATE=0.15,\n",
    "                LAMBDA=0.001,\n",
    "                NUM_EPOCHES=10,\n",
    "                BATCH_SIZE=64,\n",
    "                NUM_NEURONS=625):\n",
    "\n",
    "    # Khai báo bộ trọng số W và bias của hidden layer và output layer\n",
    "    Wh = np.random.uniform(size=(IMAGE_PIXELS, NUM_NEURONS))\n",
    "    Wo = np.random.uniform(size=(NUM_NEURONS, NUM_CLASSES))\n",
    "    Bh = np.ones((1, NUM_NEURONS))\n",
    "    Bo = np.ones((1, NUM_CLASSES))\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        # Tính tỉ lệ học theo epoch\n",
    "        learning_rate = LEARNING_RATE / (epoch + 1)\n",
    "        \n",
    "        num_batches = mnist.train.num_examples // BATCH_SIZE\n",
    "        for i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            \n",
    "            # Bước forward propagation\n",
    "            Zh = batch_xs.dot(Wh) + Bh\n",
    "            Ah = sigmoid(Zh)\n",
    "            \n",
    "            Zo = Ah.dot(Wo) + Bo\n",
    "            Ao = softmax(Zo)\n",
    "            \n",
    "            # Sử dụng cross-entropy để tính khoảng cách giữa hai phân bố xác suất\n",
    "            l = -np.sum(batch_ys * np.log(Ao))\n",
    "            # Sử dụng regularization để tránh overfitting\n",
    "            l += LAMBDA / 2 * (np.sum(np.square(Wh)) + np.sum(np.square(Wo)))\n",
    "            l /= BATCH_SIZE\n",
    "            \n",
    "            # Bước back propagation\n",
    "            # Tính độ lệch giữa vector output layer và vector nhãn của ảnh\n",
    "            Do = (Ao - batch_ys) / BATCH_SIZE\n",
    "            dWo = (Ah.T).dot(Do)\n",
    "            dBo = np.sum(Do, axis=0, keepdims=True)\n",
    "            \n",
    "            # Tính độ lệch giữa hidden layer và output layer\n",
    "            Dh = Do.dot(Wo.T) * sigmoid_prime(Ah)\n",
    "            dWh = (batch_xs.T).dot(Dh)\n",
    "            dBh = np.sum(Dh, axis=0, keepdims=True)\n",
    "            \n",
    "            # Chỉ cộng regularization cho trọng số W (không tính cho bias)\n",
    "            dWo += LAMBDA * Wo\n",
    "            dWh += LAMBDA * Wh\n",
    "            \n",
    "            # Cập nhật bộ trọng theo gradient\n",
    "            Wh -= learning_rate * dWh\n",
    "            Bh -= learning_rate * dBh\n",
    "            Wo -= learning_rate * dWo\n",
    "            Bo -= learning_rate * dBo\n",
    "            \n",
    "            avg_loss += l / num_batches\n",
    "\n",
    "        print('Epoch: %d - Average loss: %f' % (epoch + 1, avg_loss))\n",
    "    \n",
    "    # Tính độ chính xác của mô hình bằng Forward propagation\n",
    "    Zh = mnist.test.images.dot(Wh) + Bh\n",
    "    Ah = sigmoid(Zh)\n",
    "    Zo = Ah.dot(Wo) + Bo\n",
    "    Y_pred = softmax(Zo)\n",
    "    accuracy = np.mean(Y_pred.argmax(axis=1) == mnist.test.labels.argmax(axis=1))\n",
    "    print('Accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Thực nghiệm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Trong phần thực nghiệm, sinh viên sẽ thử nghiệm các trường hợp sau đồng thời cho cả 2 mô hình đã cài đặt ở trên:\n",
    "\n",
    "|LEARNING_RATE|LAMBDA|NUM_EPOCHES|BATCH_SIZE|NUM_NEURONS|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|0.15|0.001|10|64|625|\n",
    "|0.15|0.001|20|64|625|\n",
    "|0.5|0.001|10|64|800|\n",
    "|0.5|0.001|50|64|800|\n",
    "|0.5|0.001|100|64|800|\n",
    "|0.5|0.01|100|128|800|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:23:03.379739Z",
     "start_time": "2017-04-15T23:15:18.391340+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average loss: 22.539472\n",
      "Epoch: 2 - Average loss: 1.438772\n",
      "Epoch: 3 - Average loss: 1.204928\n",
      "Epoch: 4 - Average loss: 1.099773\n",
      "Epoch: 5 - Average loss: 1.035490\n",
      "Epoch: 6 - Average loss: 0.989169\n",
      "Epoch: 7 - Average loss: 0.954900\n",
      "Epoch: 8 - Average loss: 0.927398\n",
      "Epoch: 9 - Average loss: 0.903989\n",
      "Epoch: 10 - Average loss: 0.884199\n",
      "Accuracy: 0.902600\n",
      "Epoch: 1 - Average loss: 5.663398\n",
      "Epoch: 2 - Average loss: 4.617252\n",
      "Epoch: 3 - Average loss: 4.482999\n",
      "Epoch: 4 - Average loss: 4.414029\n",
      "Epoch: 5 - Average loss: 4.369126\n",
      "Epoch: 6 - Average loss: 4.338417\n",
      "Epoch: 7 - Average loss: 4.314091\n",
      "Epoch: 8 - Average loss: 4.294353\n",
      "Epoch: 9 - Average loss: 4.278798\n",
      "Epoch: 10 - Average loss: 4.265497\n",
      "Accuracy: 0.901400\n"
     ]
    }
   ],
   "source": [
    "feedforward()\n",
    "feedforward_with_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:41:12.523994Z",
     "start_time": "2017-04-15T23:25:21.637116+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average loss: 23.199943\n",
      "Epoch: 2 - Average loss: 1.419865\n",
      "Epoch: 3 - Average loss: 1.188474\n",
      "Epoch: 4 - Average loss: 1.085249\n",
      "Epoch: 5 - Average loss: 1.018902\n",
      "Epoch: 6 - Average loss: 0.974557\n",
      "Epoch: 7 - Average loss: 0.940356\n",
      "Epoch: 8 - Average loss: 0.912668\n",
      "Epoch: 9 - Average loss: 0.890290\n",
      "Epoch: 10 - Average loss: 0.871628\n",
      "Epoch: 11 - Average loss: 0.854816\n",
      "Epoch: 12 - Average loss: 0.840740\n",
      "Epoch: 13 - Average loss: 0.827877\n",
      "Epoch: 14 - Average loss: 0.816222\n",
      "Epoch: 15 - Average loss: 0.805704\n",
      "Epoch: 16 - Average loss: 0.796413\n",
      "Epoch: 17 - Average loss: 0.787479\n",
      "Epoch: 18 - Average loss: 0.779330\n",
      "Epoch: 19 - Average loss: 0.771874\n",
      "Epoch: 20 - Average loss: 0.765134\n",
      "Accuracy: 0.910500\n",
      "Epoch: 1 - Average loss: 5.737067\n",
      "Epoch: 2 - Average loss: 4.653694\n",
      "Epoch: 3 - Average loss: 4.508295\n",
      "Epoch: 4 - Average loss: 4.433784\n",
      "Epoch: 5 - Average loss: 4.386568\n",
      "Epoch: 6 - Average loss: 4.352578\n",
      "Epoch: 7 - Average loss: 4.327379\n",
      "Epoch: 8 - Average loss: 4.306161\n",
      "Epoch: 9 - Average loss: 4.288959\n",
      "Epoch: 10 - Average loss: 4.274410\n",
      "Epoch: 11 - Average loss: 4.261969\n",
      "Epoch: 12 - Average loss: 4.251346\n",
      "Epoch: 13 - Average loss: 4.241145\n",
      "Epoch: 14 - Average loss: 4.232559\n",
      "Epoch: 15 - Average loss: 4.224587\n",
      "Epoch: 16 - Average loss: 4.217482\n",
      "Epoch: 17 - Average loss: 4.211187\n",
      "Epoch: 18 - Average loss: 4.204919\n",
      "Epoch: 19 - Average loss: 4.199379\n",
      "Epoch: 20 - Average loss: 4.194297\n",
      "Accuracy: 0.903000\n"
     ]
    }
   ],
   "source": [
    "feedforward(NUM_EPOCHES=20)\n",
    "feedforward_with_tensorflow(NUM_EPOCHES=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T16:59:27.341888Z",
     "start_time": "2017-04-15T23:44:16.602523+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average loss: 14.283970\n",
      "Epoch: 2 - Average loss: 0.853750\n",
      "Epoch: 3 - Average loss: 0.643040\n",
      "Epoch: 4 - Average loss: 0.549784\n",
      "Epoch: 5 - Average loss: 0.496468\n",
      "Epoch: 6 - Average loss: 0.460909\n",
      "Epoch: 7 - Average loss: 0.436910\n",
      "Epoch: 8 - Average loss: 0.417751\n",
      "Epoch: 9 - Average loss: 0.403337\n",
      "Epoch: 10 - Average loss: 0.391768\n",
      "Accuracy: 0.927200\n",
      "Epoch: 1 - Average loss: 6.680889\n",
      "Epoch: 2 - Average loss: 5.348352\n",
      "Epoch: 3 - Average loss: 5.166866\n",
      "Epoch: 4 - Average loss: 5.081441\n",
      "Epoch: 5 - Average loss: 5.027127\n",
      "Epoch: 6 - Average loss: 4.990995\n",
      "Epoch: 7 - Average loss: 4.963470\n",
      "Epoch: 8 - Average loss: 4.941231\n",
      "Epoch: 9 - Average loss: 4.923312\n",
      "Epoch: 10 - Average loss: 4.907904\n",
      "Accuracy: 0.932500\n"
     ]
    }
   ],
   "source": [
    "feedforward(LEARNING_RATE=0.5, NUM_NEURONS=800)\n",
    "feedforward_with_tensorflow(LEARNING_RATE=0.5, NUM_NEURONS=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T17:59:05.617606Z",
     "start_time": "2017-04-16T00:05:40.683571+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average loss: 14.112744\n",
      "Epoch: 2 - Average loss: 0.851471\n",
      "Epoch: 3 - Average loss: 0.643836\n",
      "Epoch: 4 - Average loss: 0.551100\n",
      "Epoch: 5 - Average loss: 0.498097\n",
      "Epoch: 6 - Average loss: 0.463158\n",
      "Epoch: 7 - Average loss: 0.438690\n",
      "Epoch: 8 - Average loss: 0.419955\n",
      "Epoch: 9 - Average loss: 0.404750\n",
      "Epoch: 10 - Average loss: 0.393350\n",
      "Epoch: 11 - Average loss: 0.384036\n",
      "Epoch: 12 - Average loss: 0.375822\n",
      "Epoch: 13 - Average loss: 0.368937\n",
      "Epoch: 14 - Average loss: 0.363116\n",
      "Epoch: 15 - Average loss: 0.357961\n",
      "Epoch: 16 - Average loss: 0.353381\n",
      "Epoch: 17 - Average loss: 0.349354\n",
      "Epoch: 18 - Average loss: 0.345548\n",
      "Epoch: 19 - Average loss: 0.342800\n",
      "Epoch: 20 - Average loss: 0.339812\n",
      "Epoch: 21 - Average loss: 0.337014\n",
      "Epoch: 22 - Average loss: 0.334816\n",
      "Epoch: 23 - Average loss: 0.332724\n",
      "Epoch: 24 - Average loss: 0.330427\n",
      "Epoch: 25 - Average loss: 0.328853\n",
      "Epoch: 26 - Average loss: 0.327006\n",
      "Epoch: 27 - Average loss: 0.325406\n",
      "Epoch: 28 - Average loss: 0.324019\n",
      "Epoch: 29 - Average loss: 0.322894\n",
      "Epoch: 30 - Average loss: 0.321555\n",
      "Epoch: 31 - Average loss: 0.320125\n",
      "Epoch: 32 - Average loss: 0.319392\n",
      "Epoch: 33 - Average loss: 0.318426\n",
      "Epoch: 34 - Average loss: 0.317351\n",
      "Epoch: 35 - Average loss: 0.316382\n",
      "Epoch: 36 - Average loss: 0.315553\n",
      "Epoch: 37 - Average loss: 0.314821\n",
      "Epoch: 38 - Average loss: 0.314054\n",
      "Epoch: 39 - Average loss: 0.313152\n",
      "Epoch: 40 - Average loss: 0.312501\n",
      "Epoch: 41 - Average loss: 0.311831\n",
      "Epoch: 42 - Average loss: 0.311313\n",
      "Epoch: 43 - Average loss: 0.310727\n",
      "Epoch: 44 - Average loss: 0.310235\n",
      "Epoch: 45 - Average loss: 0.309503\n",
      "Epoch: 46 - Average loss: 0.309143\n",
      "Epoch: 47 - Average loss: 0.308696\n",
      "Epoch: 48 - Average loss: 0.308113\n",
      "Epoch: 49 - Average loss: 0.307805\n",
      "Epoch: 50 - Average loss: 0.307235\n",
      "Accuracy: 0.925400\n",
      "Epoch: 1 - Average loss: 6.663311\n",
      "Epoch: 2 - Average loss: 5.349945\n",
      "Epoch: 3 - Average loss: 5.171870\n",
      "Epoch: 4 - Average loss: 5.087378\n",
      "Epoch: 5 - Average loss: 5.035280\n",
      "Epoch: 6 - Average loss: 4.997736\n",
      "Epoch: 7 - Average loss: 4.969511\n",
      "Epoch: 8 - Average loss: 4.947579\n",
      "Epoch: 9 - Average loss: 4.929246\n",
      "Epoch: 10 - Average loss: 4.913854\n",
      "Epoch: 11 - Average loss: 4.900650\n",
      "Epoch: 12 - Average loss: 4.888448\n",
      "Epoch: 13 - Average loss: 4.878377\n",
      "Epoch: 14 - Average loss: 4.869018\n",
      "Epoch: 15 - Average loss: 4.860634\n",
      "Epoch: 16 - Average loss: 4.852943\n",
      "Epoch: 17 - Average loss: 4.845711\n",
      "Epoch: 18 - Average loss: 4.839191\n",
      "Epoch: 19 - Average loss: 4.832656\n",
      "Epoch: 20 - Average loss: 4.827120\n",
      "Epoch: 21 - Average loss: 4.822042\n",
      "Epoch: 22 - Average loss: 4.816962\n",
      "Epoch: 23 - Average loss: 4.812164\n",
      "Epoch: 24 - Average loss: 4.807538\n",
      "Epoch: 25 - Average loss: 4.803440\n",
      "Epoch: 26 - Average loss: 4.799438\n",
      "Epoch: 27 - Average loss: 4.795477\n",
      "Epoch: 28 - Average loss: 4.791930\n",
      "Epoch: 29 - Average loss: 4.788283\n",
      "Epoch: 30 - Average loss: 4.784868\n",
      "Epoch: 31 - Average loss: 4.781769\n",
      "Epoch: 32 - Average loss: 4.778522\n",
      "Epoch: 33 - Average loss: 4.775694\n",
      "Epoch: 34 - Average loss: 4.772834\n",
      "Epoch: 35 - Average loss: 4.769968\n",
      "Epoch: 36 - Average loss: 4.767305\n",
      "Epoch: 37 - Average loss: 4.764793\n",
      "Epoch: 38 - Average loss: 4.762166\n",
      "Epoch: 39 - Average loss: 4.759865\n",
      "Epoch: 40 - Average loss: 4.757325\n",
      "Epoch: 41 - Average loss: 4.755121\n",
      "Epoch: 42 - Average loss: 4.752591\n",
      "Epoch: 43 - Average loss: 4.750647\n",
      "Epoch: 44 - Average loss: 4.748444\n",
      "Epoch: 45 - Average loss: 4.746349\n",
      "Epoch: 46 - Average loss: 4.744294\n",
      "Epoch: 47 - Average loss: 4.742310\n",
      "Epoch: 48 - Average loss: 4.740430\n",
      "Epoch: 49 - Average loss: 4.738486\n",
      "Epoch: 50 - Average loss: 4.736761\n",
      "Accuracy: 0.937500\n"
     ]
    }
   ],
   "source": [
    "feedforward(LEARNING_RATE=0.5, NUM_EPOCHES=50, NUM_NEURONS=800)\n",
    "feedforward_with_tensorflow(LEARNING_RATE=0.5, NUM_EPOCHES=50, NUM_NEURONS=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-15T19:30:37.268288Z",
     "start_time": "2017-04-16T00:59:05.647629+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average loss: 14.335291\n",
      "Epoch: 2 - Average loss: 0.857940\n",
      "Epoch: 3 - Average loss: 0.645431\n",
      "Epoch: 4 - Average loss: 0.550595\n",
      "Epoch: 5 - Average loss: 0.498022\n",
      "Epoch: 6 - Average loss: 0.462677\n",
      "Epoch: 7 - Average loss: 0.437814\n",
      "Epoch: 8 - Average loss: 0.418466\n",
      "Epoch: 9 - Average loss: 0.403474\n",
      "Epoch: 10 - Average loss: 0.392097\n",
      "Epoch: 11 - Average loss: 0.382340\n",
      "Epoch: 12 - Average loss: 0.374290\n",
      "Epoch: 13 - Average loss: 0.367459\n",
      "Epoch: 14 - Average loss: 0.361558\n",
      "Epoch: 15 - Average loss: 0.356196\n",
      "Epoch: 16 - Average loss: 0.351920\n",
      "Epoch: 17 - Average loss: 0.347986\n",
      "Epoch: 18 - Average loss: 0.344534\n",
      "Epoch: 19 - Average loss: 0.341203\n",
      "Epoch: 20 - Average loss: 0.338345\n",
      "Epoch: 21 - Average loss: 0.335975\n",
      "Epoch: 22 - Average loss: 0.333529\n",
      "Epoch: 23 - Average loss: 0.331167\n",
      "Epoch: 24 - Average loss: 0.329384\n",
      "Epoch: 25 - Average loss: 0.327782\n",
      "Epoch: 26 - Average loss: 0.326183\n",
      "Epoch: 27 - Average loss: 0.324444\n",
      "Epoch: 28 - Average loss: 0.323149\n",
      "Epoch: 29 - Average loss: 0.321631\n",
      "Epoch: 30 - Average loss: 0.320570\n",
      "Epoch: 31 - Average loss: 0.319325\n",
      "Epoch: 32 - Average loss: 0.318401\n",
      "Epoch: 33 - Average loss: 0.317307\n",
      "Epoch: 34 - Average loss: 0.316397\n",
      "Epoch: 35 - Average loss: 0.315488\n",
      "Epoch: 36 - Average loss: 0.314627\n",
      "Epoch: 37 - Average loss: 0.313906\n",
      "Epoch: 38 - Average loss: 0.313151\n",
      "Epoch: 39 - Average loss: 0.312605\n",
      "Epoch: 40 - Average loss: 0.311779\n",
      "Epoch: 41 - Average loss: 0.311247\n",
      "Epoch: 42 - Average loss: 0.310533\n",
      "Epoch: 43 - Average loss: 0.309884\n",
      "Epoch: 44 - Average loss: 0.309670\n",
      "Epoch: 45 - Average loss: 0.309145\n",
      "Epoch: 46 - Average loss: 0.308483\n",
      "Epoch: 47 - Average loss: 0.308117\n",
      "Epoch: 48 - Average loss: 0.307608\n",
      "Epoch: 49 - Average loss: 0.307200\n",
      "Epoch: 50 - Average loss: 0.306983\n",
      "Epoch: 51 - Average loss: 0.306439\n",
      "Epoch: 52 - Average loss: 0.306025\n",
      "Epoch: 53 - Average loss: 0.305839\n",
      "Epoch: 54 - Average loss: 0.305249\n",
      "Epoch: 55 - Average loss: 0.305085\n",
      "Epoch: 56 - Average loss: 0.304902\n",
      "Epoch: 57 - Average loss: 0.304396\n",
      "Epoch: 58 - Average loss: 0.304224\n",
      "Epoch: 59 - Average loss: 0.303750\n",
      "Epoch: 60 - Average loss: 0.303542\n",
      "Epoch: 61 - Average loss: 0.303367\n",
      "Epoch: 62 - Average loss: 0.303074\n",
      "Epoch: 63 - Average loss: 0.302960\n",
      "Epoch: 64 - Average loss: 0.302702\n",
      "Epoch: 65 - Average loss: 0.302482\n",
      "Epoch: 66 - Average loss: 0.302102\n",
      "Epoch: 67 - Average loss: 0.302151\n",
      "Epoch: 68 - Average loss: 0.301859\n",
      "Epoch: 69 - Average loss: 0.301587\n",
      "Epoch: 70 - Average loss: 0.301452\n",
      "Epoch: 71 - Average loss: 0.301259\n",
      "Epoch: 72 - Average loss: 0.301075\n",
      "Epoch: 73 - Average loss: 0.300781\n",
      "Epoch: 74 - Average loss: 0.300760\n",
      "Epoch: 75 - Average loss: 0.300747\n",
      "Epoch: 76 - Average loss: 0.300436\n",
      "Epoch: 77 - Average loss: 0.300343\n",
      "Epoch: 78 - Average loss: 0.300133\n",
      "Epoch: 79 - Average loss: 0.299897\n",
      "Epoch: 80 - Average loss: 0.299858\n",
      "Epoch: 81 - Average loss: 0.299777\n",
      "Epoch: 82 - Average loss: 0.299579\n",
      "Epoch: 83 - Average loss: 0.299418\n",
      "Epoch: 84 - Average loss: 0.299304\n",
      "Epoch: 85 - Average loss: 0.299266\n",
      "Epoch: 86 - Average loss: 0.299102\n",
      "Epoch: 87 - Average loss: 0.299064\n",
      "Epoch: 88 - Average loss: 0.298945\n",
      "Epoch: 89 - Average loss: 0.298823\n",
      "Epoch: 90 - Average loss: 0.298661\n",
      "Epoch: 91 - Average loss: 0.298638\n",
      "Epoch: 92 - Average loss: 0.298516\n",
      "Epoch: 93 - Average loss: 0.298460\n",
      "Epoch: 94 - Average loss: 0.298255\n",
      "Epoch: 95 - Average loss: 0.298278\n",
      "Epoch: 96 - Average loss: 0.298124\n",
      "Epoch: 97 - Average loss: 0.298044\n",
      "Epoch: 98 - Average loss: 0.297850\n",
      "Epoch: 99 - Average loss: 0.297964\n",
      "Epoch: 100 - Average loss: 0.297751\n",
      "Accuracy: 0.924100\n",
      "Epoch: 1 - Average loss: 6.632811\n",
      "Epoch: 2 - Average loss: 5.323686\n",
      "Epoch: 3 - Average loss: 5.145439\n",
      "Epoch: 4 - Average loss: 5.061884\n",
      "Epoch: 5 - Average loss: 5.008997\n",
      "Epoch: 6 - Average loss: 4.973380\n",
      "Epoch: 7 - Average loss: 4.945096\n",
      "Epoch: 8 - Average loss: 4.923275\n",
      "Epoch: 9 - Average loss: 4.905685\n",
      "Epoch: 10 - Average loss: 4.889952\n",
      "Epoch: 11 - Average loss: 4.876915\n",
      "Epoch: 12 - Average loss: 4.865470\n",
      "Epoch: 13 - Average loss: 4.855320\n",
      "Epoch: 14 - Average loss: 4.846108\n",
      "Epoch: 15 - Average loss: 4.837396\n",
      "Epoch: 16 - Average loss: 4.830292\n",
      "Epoch: 17 - Average loss: 4.823077\n",
      "Epoch: 18 - Average loss: 4.816602\n",
      "Epoch: 19 - Average loss: 4.810602\n",
      "Epoch: 20 - Average loss: 4.804942\n",
      "Epoch: 21 - Average loss: 4.799691\n",
      "Epoch: 22 - Average loss: 4.794627\n",
      "Epoch: 23 - Average loss: 4.790057\n",
      "Epoch: 24 - Average loss: 4.785468\n",
      "Epoch: 25 - Average loss: 4.781486\n",
      "Epoch: 26 - Average loss: 4.777343\n",
      "Epoch: 27 - Average loss: 4.773708\n",
      "Epoch: 28 - Average loss: 4.770004\n",
      "Epoch: 29 - Average loss: 4.766641\n",
      "Epoch: 30 - Average loss: 4.763284\n",
      "Epoch: 31 - Average loss: 4.760160\n",
      "Epoch: 32 - Average loss: 4.756978\n",
      "Epoch: 33 - Average loss: 4.754118\n",
      "Epoch: 34 - Average loss: 4.751328\n",
      "Epoch: 35 - Average loss: 4.748625\n",
      "Epoch: 36 - Average loss: 4.745947\n",
      "Epoch: 37 - Average loss: 4.743424\n",
      "Epoch: 38 - Average loss: 4.740988\n",
      "Epoch: 39 - Average loss: 4.738573\n",
      "Epoch: 40 - Average loss: 4.736298\n",
      "Epoch: 41 - Average loss: 4.733974\n",
      "Epoch: 42 - Average loss: 4.731732\n",
      "Epoch: 43 - Average loss: 4.729540\n",
      "Epoch: 44 - Average loss: 4.727473\n",
      "Epoch: 45 - Average loss: 4.725366\n",
      "Epoch: 46 - Average loss: 4.723070\n",
      "Epoch: 47 - Average loss: 4.721410\n",
      "Epoch: 48 - Average loss: 4.719543\n",
      "Epoch: 49 - Average loss: 4.717718\n",
      "Epoch: 50 - Average loss: 4.715958\n",
      "Epoch: 51 - Average loss: 4.714255\n",
      "Epoch: 52 - Average loss: 4.712579\n",
      "Epoch: 53 - Average loss: 4.711027\n",
      "Epoch: 54 - Average loss: 4.709409\n",
      "Epoch: 55 - Average loss: 4.707821\n",
      "Epoch: 56 - Average loss: 4.706177\n",
      "Epoch: 57 - Average loss: 4.704821\n",
      "Epoch: 58 - Average loss: 4.703425\n",
      "Epoch: 59 - Average loss: 4.701964\n",
      "Epoch: 60 - Average loss: 4.700580\n",
      "Epoch: 61 - Average loss: 4.699126\n",
      "Epoch: 62 - Average loss: 4.697868\n",
      "Epoch: 63 - Average loss: 4.696542\n",
      "Epoch: 64 - Average loss: 4.695225\n",
      "Epoch: 65 - Average loss: 4.693799\n",
      "Epoch: 66 - Average loss: 4.692684\n",
      "Epoch: 67 - Average loss: 4.691479\n",
      "Epoch: 68 - Average loss: 4.690252\n",
      "Epoch: 69 - Average loss: 4.689092\n",
      "Epoch: 70 - Average loss: 4.687912\n",
      "Epoch: 71 - Average loss: 4.686794\n",
      "Epoch: 72 - Average loss: 4.685577\n",
      "Epoch: 73 - Average loss: 4.684516\n",
      "Epoch: 74 - Average loss: 4.683320\n",
      "Epoch: 75 - Average loss: 4.682313\n",
      "Epoch: 76 - Average loss: 4.681149\n",
      "Epoch: 77 - Average loss: 4.680217\n",
      "Epoch: 78 - Average loss: 4.679161\n",
      "Epoch: 79 - Average loss: 4.678197\n",
      "Epoch: 80 - Average loss: 4.677090\n",
      "Epoch: 81 - Average loss: 4.676164\n",
      "Epoch: 82 - Average loss: 4.675184\n",
      "Epoch: 83 - Average loss: 4.674198\n",
      "Epoch: 84 - Average loss: 4.673238\n",
      "Epoch: 85 - Average loss: 4.672298\n",
      "Epoch: 86 - Average loss: 4.671397\n",
      "Epoch: 87 - Average loss: 4.670500\n",
      "Epoch: 88 - Average loss: 4.669510\n",
      "Epoch: 89 - Average loss: 4.668671\n",
      "Epoch: 90 - Average loss: 4.667782\n",
      "Epoch: 91 - Average loss: 4.666844\n",
      "Epoch: 92 - Average loss: 4.666028\n",
      "Epoch: 93 - Average loss: 4.665168\n",
      "Epoch: 94 - Average loss: 4.664291\n",
      "Epoch: 95 - Average loss: 4.663449\n",
      "Epoch: 96 - Average loss: 4.662587\n",
      "Epoch: 97 - Average loss: 4.661725\n",
      "Epoch: 98 - Average loss: 4.660887\n",
      "Epoch: 99 - Average loss: 4.660057\n",
      "Epoch: 100 - Average loss: 4.659194\n",
      "Accuracy: 0.940500\n"
     ]
    }
   ],
   "source": [
    "feedforward(LEARNING_RATE=0.5, NUM_EPOCHES=100, NUM_NEURONS=800)\n",
    "feedforward_with_tensorflow(LEARNING_RATE=0.5, NUM_EPOCHES=100, NUM_NEURONS=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T03:56:48.245848Z",
     "start_time": "2017-04-16T09:35:57.522415+07:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average loss: 24.237172\n",
      "Epoch: 2 - Average loss: 0.566677\n",
      "Epoch: 3 - Average loss: 0.542913\n",
      "Epoch: 4 - Average loss: 0.538591\n",
      "Epoch: 5 - Average loss: 0.536267\n",
      "Epoch: 6 - Average loss: 0.535305\n",
      "Epoch: 7 - Average loss: 0.534270\n",
      "Epoch: 8 - Average loss: 0.533833\n",
      "Epoch: 9 - Average loss: 0.533464\n",
      "Epoch: 10 - Average loss: 0.533380\n",
      "Epoch: 11 - Average loss: 0.532849\n",
      "Epoch: 12 - Average loss: 0.533136\n",
      "Epoch: 13 - Average loss: 0.532668\n",
      "Epoch: 14 - Average loss: 0.532282\n",
      "Epoch: 15 - Average loss: 0.533041\n",
      "Epoch: 16 - Average loss: 0.532186\n",
      "Epoch: 17 - Average loss: 0.532748\n",
      "Epoch: 18 - Average loss: 0.532064\n",
      "Epoch: 19 - Average loss: 0.532535\n",
      "Epoch: 20 - Average loss: 0.531954\n",
      "Epoch: 21 - Average loss: 0.532066\n",
      "Epoch: 22 - Average loss: 0.532023\n",
      "Epoch: 23 - Average loss: 0.531650\n",
      "Epoch: 24 - Average loss: 0.531792\n",
      "Epoch: 25 - Average loss: 0.531709\n",
      "Epoch: 26 - Average loss: 0.531877\n",
      "Epoch: 27 - Average loss: 0.532071\n",
      "Epoch: 28 - Average loss: 0.531267\n",
      "Epoch: 29 - Average loss: 0.531553\n",
      "Epoch: 30 - Average loss: 0.531625\n",
      "Epoch: 31 - Average loss: 0.531337\n",
      "Epoch: 32 - Average loss: 0.531698\n",
      "Epoch: 33 - Average loss: 0.531450\n",
      "Epoch: 34 - Average loss: 0.531598\n",
      "Epoch: 35 - Average loss: 0.531755\n",
      "Epoch: 36 - Average loss: 0.531814\n",
      "Epoch: 37 - Average loss: 0.531733\n",
      "Epoch: 38 - Average loss: 0.531699\n",
      "Epoch: 39 - Average loss: 0.531304\n",
      "Epoch: 40 - Average loss: 0.531480\n",
      "Epoch: 41 - Average loss: 0.531346\n",
      "Epoch: 42 - Average loss: 0.531582\n",
      "Epoch: 43 - Average loss: 0.531365\n",
      "Epoch: 44 - Average loss: 0.531434\n",
      "Epoch: 45 - Average loss: 0.531496\n",
      "Epoch: 46 - Average loss: 0.531333\n",
      "Epoch: 47 - Average loss: 0.531506\n",
      "Epoch: 48 - Average loss: 0.531426\n",
      "Epoch: 49 - Average loss: 0.531302\n",
      "Epoch: 50 - Average loss: 0.531550\n",
      "Epoch: 51 - Average loss: 0.531481\n",
      "Epoch: 52 - Average loss: 0.531346\n",
      "Epoch: 53 - Average loss: 0.531435\n",
      "Epoch: 54 - Average loss: 0.531341\n",
      "Epoch: 55 - Average loss: 0.531423\n",
      "Epoch: 56 - Average loss: 0.531332\n",
      "Epoch: 57 - Average loss: 0.531380\n",
      "Epoch: 58 - Average loss: 0.531395\n",
      "Epoch: 59 - Average loss: 0.531294\n",
      "Epoch: 60 - Average loss: 0.531498\n",
      "Epoch: 61 - Average loss: 0.531103\n",
      "Epoch: 62 - Average loss: 0.531279\n",
      "Epoch: 63 - Average loss: 0.531480\n",
      "Epoch: 64 - Average loss: 0.531386\n",
      "Epoch: 65 - Average loss: 0.531295\n",
      "Epoch: 66 - Average loss: 0.531173\n",
      "Epoch: 67 - Average loss: 0.531286\n",
      "Epoch: 68 - Average loss: 0.531213\n",
      "Epoch: 69 - Average loss: 0.531247\n",
      "Epoch: 70 - Average loss: 0.531273\n",
      "Epoch: 71 - Average loss: 0.531136\n",
      "Epoch: 72 - Average loss: 0.531260\n",
      "Epoch: 73 - Average loss: 0.531289\n",
      "Epoch: 74 - Average loss: 0.531352\n",
      "Epoch: 75 - Average loss: 0.531256\n",
      "Epoch: 76 - Average loss: 0.531017\n",
      "Epoch: 77 - Average loss: 0.531163\n",
      "Epoch: 78 - Average loss: 0.531270\n",
      "Epoch: 79 - Average loss: 0.531225\n",
      "Epoch: 80 - Average loss: 0.531086\n",
      "Epoch: 81 - Average loss: 0.531228\n",
      "Epoch: 82 - Average loss: 0.531237\n",
      "Epoch: 83 - Average loss: 0.531200\n",
      "Epoch: 84 - Average loss: 0.531074\n",
      "Epoch: 85 - Average loss: 0.530914\n",
      "Epoch: 86 - Average loss: 0.531154\n",
      "Epoch: 87 - Average loss: 0.531056\n",
      "Epoch: 88 - Average loss: 0.531337\n",
      "Epoch: 89 - Average loss: 0.531112\n",
      "Epoch: 90 - Average loss: 0.531326\n",
      "Epoch: 91 - Average loss: 0.531183\n",
      "Epoch: 92 - Average loss: 0.531267\n",
      "Epoch: 93 - Average loss: 0.531151\n",
      "Epoch: 94 - Average loss: 0.531323\n",
      "Epoch: 95 - Average loss: 0.531147\n",
      "Epoch: 96 - Average loss: 0.531061\n",
      "Epoch: 97 - Average loss: 0.531141\n",
      "Epoch: 98 - Average loss: 0.530994\n",
      "Epoch: 99 - Average loss: 0.531205\n",
      "Epoch: 100 - Average loss: 0.531461\n",
      "Accuracy: 0.893300\n",
      "Epoch: 1 - Average loss: 26.484723\n",
      "Epoch: 2 - Average loss: 24.404019\n",
      "Epoch: 3 - Average loss: 23.919050\n",
      "Epoch: 4 - Average loss: 23.618702\n",
      "Epoch: 5 - Average loss: 23.397873\n",
      "Epoch: 6 - Average loss: 23.225441\n",
      "Epoch: 7 - Average loss: 23.084535\n",
      "Epoch: 8 - Average loss: 22.965021\n",
      "Epoch: 9 - Average loss: 22.861144\n",
      "Epoch: 10 - Average loss: 22.768871\n",
      "Epoch: 11 - Average loss: 22.687410\n",
      "Epoch: 12 - Average loss: 22.612967\n",
      "Epoch: 13 - Average loss: 22.545102\n",
      "Epoch: 14 - Average loss: 22.483131\n",
      "Epoch: 15 - Average loss: 22.425340\n",
      "Epoch: 16 - Average loss: 22.371826\n",
      "Epoch: 17 - Average loss: 22.322125\n",
      "Epoch: 18 - Average loss: 22.275104\n",
      "Epoch: 19 - Average loss: 22.230748\n",
      "Epoch: 20 - Average loss: 22.188769\n",
      "Epoch: 21 - Average loss: 22.150195\n",
      "Epoch: 22 - Average loss: 22.111899\n",
      "Epoch: 23 - Average loss: 22.076564\n",
      "Epoch: 24 - Average loss: 22.042474\n",
      "Epoch: 25 - Average loss: 22.010110\n",
      "Epoch: 26 - Average loss: 21.977985\n",
      "Epoch: 27 - Average loss: 21.948785\n",
      "Epoch: 28 - Average loss: 21.920044\n",
      "Epoch: 29 - Average loss: 21.891736\n",
      "Epoch: 30 - Average loss: 21.865210\n",
      "Epoch: 31 - Average loss: 21.839592\n",
      "Epoch: 32 - Average loss: 21.814920\n",
      "Epoch: 33 - Average loss: 21.790758\n",
      "Epoch: 34 - Average loss: 21.767710\n",
      "Epoch: 35 - Average loss: 21.744803\n",
      "Epoch: 36 - Average loss: 21.723011\n",
      "Epoch: 37 - Average loss: 21.701827\n",
      "Epoch: 38 - Average loss: 21.681090\n",
      "Epoch: 39 - Average loss: 21.660689\n",
      "Epoch: 40 - Average loss: 21.641357\n",
      "Epoch: 41 - Average loss: 21.622357\n",
      "Epoch: 42 - Average loss: 21.603366\n",
      "Epoch: 43 - Average loss: 21.585515\n",
      "Epoch: 44 - Average loss: 21.567912\n",
      "Epoch: 45 - Average loss: 21.550236\n",
      "Epoch: 46 - Average loss: 21.533945\n",
      "Epoch: 47 - Average loss: 21.517196\n",
      "Epoch: 48 - Average loss: 21.501063\n",
      "Epoch: 49 - Average loss: 21.485146\n",
      "Epoch: 50 - Average loss: 21.469453\n",
      "Epoch: 51 - Average loss: 21.455016\n",
      "Epoch: 52 - Average loss: 21.440071\n",
      "Epoch: 53 - Average loss: 21.425616\n",
      "Epoch: 54 - Average loss: 21.411531\n",
      "Epoch: 55 - Average loss: 21.397364\n",
      "Epoch: 56 - Average loss: 21.383677\n",
      "Epoch: 57 - Average loss: 21.370693\n",
      "Epoch: 58 - Average loss: 21.357530\n",
      "Epoch: 59 - Average loss: 21.343962\n",
      "Epoch: 60 - Average loss: 21.331762\n",
      "Epoch: 61 - Average loss: 21.319429\n",
      "Epoch: 62 - Average loss: 21.307109\n",
      "Epoch: 63 - Average loss: 21.294803\n",
      "Epoch: 64 - Average loss: 21.283105\n",
      "Epoch: 65 - Average loss: 21.270968\n",
      "Epoch: 66 - Average loss: 21.260181\n",
      "Epoch: 67 - Average loss: 21.248872\n",
      "Epoch: 68 - Average loss: 21.237749\n",
      "Epoch: 69 - Average loss: 21.226792\n",
      "Epoch: 70 - Average loss: 21.216203\n",
      "Epoch: 71 - Average loss: 21.205481\n",
      "Epoch: 72 - Average loss: 21.195030\n",
      "Epoch: 73 - Average loss: 21.184690\n",
      "Epoch: 74 - Average loss: 21.174589\n",
      "Epoch: 75 - Average loss: 21.163919\n",
      "Epoch: 76 - Average loss: 21.154243\n",
      "Epoch: 77 - Average loss: 21.144925\n",
      "Epoch: 78 - Average loss: 21.135233\n",
      "Epoch: 79 - Average loss: 21.125873\n",
      "Epoch: 80 - Average loss: 21.116367\n",
      "Epoch: 81 - Average loss: 21.107326\n",
      "Epoch: 82 - Average loss: 21.098055\n",
      "Epoch: 83 - Average loss: 21.089078\n",
      "Epoch: 84 - Average loss: 21.080395\n",
      "Epoch: 85 - Average loss: 21.071406\n",
      "Epoch: 86 - Average loss: 21.062982\n",
      "Epoch: 87 - Average loss: 21.054501\n",
      "Epoch: 88 - Average loss: 21.046109\n",
      "Epoch: 89 - Average loss: 21.037346\n",
      "Epoch: 90 - Average loss: 21.029464\n",
      "Epoch: 91 - Average loss: 21.021242\n",
      "Epoch: 92 - Average loss: 21.013179\n",
      "Epoch: 93 - Average loss: 21.005155\n",
      "Epoch: 94 - Average loss: 20.997272\n",
      "Epoch: 95 - Average loss: 20.989369\n",
      "Epoch: 96 - Average loss: 20.981608\n",
      "Epoch: 97 - Average loss: 20.974036\n",
      "Epoch: 98 - Average loss: 20.966406\n",
      "Epoch: 99 - Average loss: 20.959109\n",
      "Epoch: 100 - Average loss: 20.951651\n",
      "Accuracy: 0.925300\n"
     ]
    }
   ],
   "source": [
    "feedforward(LEARNING_RATE=0.5, LAMBDA=0.01, NUM_EPOCHES=100, BATCH_SIZE=128, NUM_NEURONS=800)\n",
    "feedforward_with_tensorflow(LEARNING_RATE=0.5, LAMBDA=0.01, NUM_EPOCHES=100, BATCH_SIZE=128, NUM_NEURONS=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nhận xét\n",
    "\n",
    "1. Độ chính xác của 2 mô hình xấp xỉ nhau, điều đó cho thấy mô hình không sử dụng thư viện được cài đặt và hoạt động chính xác.\n",
    "\n",
    "1. Hàm loss của mô hình tự cài đặt có thể khác 1 chút so với hàm loss của Tensorflow mặc dù cùng là cross-entropy loss. Điều này có thể nằm ở việc nhóm sử dụng hàm softmax cải tiến (trừ đi một hằng số c nào đó để tránh overflow). Bên cạnh đó, giá trị Average Loss còn phụ thuộc vào ma trận trọng số khởi tạo ban đầu, phân bố xác suất của hàm khởi tạo, tỉ lệ học,...\n",
    "\n",
    "1. Do sử dụng chiến lược Mini-batch Gradient Descent với learning rate động nên nhìn chung hàm số không gặp khó khăn khi hội tụ. Có thể nhận thấy được bằng cách quan sát giá trị Average loss giảm dần theo số epoch, không có trường hợp bị ziczac hoặc không hội tụ.\n",
    "\n",
    "1. Có thể tăng số lượng neurons trong lớp ẩn lên để mô hình có khả năng tổng quát hóa cao hơn. Nhưng lúc đó đòi hỏi một lượng lớn dữ liệu huấn luyện để hạn chế bị overfit.\n",
    "\n",
    "1. Tuy chiến lược Mini-batch Gradient Descent nhìn có vẻ không tốt bằng Batch Gradient Descent do số điểm dữ liệu nhỏ hơn rất nhiều nhưng thực tế cho thấy chiến lược Mini-batch Gradient Descent vẫn có khả năng xấp xỉ tốt. Bởi vì trong thống kê, vẫn có thể dùng một mẫu nhỏ để xấp xỉ cho mẫu lớn nhưng cần phải chọn kích thước mẫu hợp lí và đảm bảo tính ngẫu nhiên của nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tài liệu tham khảo\n",
    "\n",
    "1. http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "1. https://www.tensorflow.org/get_started/get_started\n",
    "\n",
    "1. http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "\n",
    "1. http://machinelearningcoban.com/2017/02/17/softmax/#-softmax-regression-cho-mnist\n",
    "\n",
    "1. http://machinelearningcoban.com/2017/02/24/mlp/#-tinh-toan-backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
